{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwbXGpQhmCKNYdk7gJuucu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baah134/Baah134/blob/main/SER_CARINE/Paper_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa numpy scipy PyWavelets\n",
        "DATASET_PATH = \"/content/drive/MyDrive/DeepLearning/External/EMoDB/\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u07C48dbE7xq",
        "outputId": "8646a19f-02c3-4006-d3d2-6154504187ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (1.9.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import scipy.stats\n",
        "import pywt\n",
        "from scipy.signal import lfilter\n",
        "from tqdm import tqdm  # <--- NEW IMPORT\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP & CONFIGURATION\n",
        "# ==========================================\n",
        "DATASET_PATH = \"/content/drive/MyDrive/DeepLearning/External/EMoDB/\"\n",
        "OUTPUT_PATH = \"processed_data/\"\n",
        "\n",
        "CLASSES = ['Angry', 'Boredom', 'Disgust', 'Anxiety', 'Happiness', 'Sadness', 'Neutral']\n",
        "\n",
        "CODE_TO_EMOTION = {\n",
        "    'W': 'Angry',\n",
        "    'L': 'Boredom',\n",
        "    'E': 'Disgust',\n",
        "    'A': 'Anxiety',\n",
        "    'F': 'Happiness',\n",
        "    'T': 'Sadness',\n",
        "    'N': 'Neutral'\n",
        "}\n",
        "\n",
        "EMOTION_TO_INT = {label: i for i, label in enumerate(CLASSES)}\n",
        "\n",
        "# ==========================================\n",
        "# 2. FEATURE EXTRACTOR\n",
        "# ==========================================\n",
        "def extract_bhangale_features(audio_path):\n",
        "    \"\"\"\n",
        "    Extracts the exact 715-dim feature vector described in Bhangale et al. (2023).\n",
        "    \"\"\"\n",
        "    # [cite_start]Load audio, Resample to 16kHz [cite: 1795]\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # [cite_start]Standardize to 4 seconds (64000 samples) [cite: 1796]\n",
        "    target_length = 64000\n",
        "    if len(y) < target_length:\n",
        "        y = np.pad(y, (0, target_length - len(y)))\n",
        "    else:\n",
        "        y = y[:target_length]\n",
        "\n",
        "    # [cite_start]Pre-emphasis filter [cite: 1528]\n",
        "    y = lfilter([1, -0.97], [1], y)\n",
        "\n",
        "    # [cite_start]Frame Settings: 40ms window, 50% overlap [cite: 1529]\n",
        "    n_fft = 640\n",
        "    hop_length = 320\n",
        "\n",
        "    # --- A. TIME-SERIES FEATURES (Length 199 each) ---\n",
        "    zcr = _fix_length(librosa.feature.zero_crossing_rate(y, frame_length=n_fft, hop_length=hop_length)[0], 199)\n",
        "    centroid = _fix_length(librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)[0], 199)\n",
        "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
        "    kurtosis = _fix_length(scipy.stats.kurtosis(S, axis=0), 199)\n",
        "\n",
        "    # --- B. STATIC FEATURES ---\n",
        "    # [cite_start]MFCC (39) [cite: 1553]\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=n_fft, hop_length=hop_length)\n",
        "    mfcc_combined = np.concatenate((mfcc, librosa.feature.delta(mfcc), librosa.feature.delta(mfcc, order=2)), axis=0)\n",
        "    mfcc_global = np.mean(mfcc_combined, axis=1)\n",
        "\n",
        "    # Scalars & Stats\n",
        "    rms_global = np.array([np.mean(librosa.feature.rms(y=y, frame_length=n_fft, hop_length=hop_length)[0])])\n",
        "    rolloff_global = np.array([np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)[0])])\n",
        "\n",
        "    # [cite_start]LPCC (13) [cite: 1599]\n",
        "    lpc_coeffs = librosa.lpc(y, order=13)\n",
        "    lpcc_global = lpc_coeffs[1:]\n",
        "    if len(lpcc_global) < 13: lpcc_global = np.pad(lpcc_global, (0, 13-len(lpcc_global)))\n",
        "\n",
        "    # [cite_start]Wavelet Packet Transform (56) [cite: 1668]\n",
        "    wp = pywt.WaveletPacket(data=y, wavelet='db2', mode='symmetric', maxlevel=3)\n",
        "    wpt_features = []\n",
        "    # Note: Loop is over only 8 nodes, so no tqdm needed here (too fast)\n",
        "    for node in wp.get_level(3, 'natural'):\n",
        "        d = node.data\n",
        "        wpt_features.extend([np.mean(d), np.median(d), np.std(d), np.var(d), scipy.stats.skew(d), scipy.stats.kurtosis(d), np.sum(d**2)])\n",
        "    wpt_global = np.array(wpt_features)\n",
        "\n",
        "    # [cite_start]Voice Quality (3) & Formants (5) [cite: 1614, 1629]\n",
        "    # PyIN is the slowest part of this function\n",
        "    f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "    f0 = f0[~np.isnan(f0)]\n",
        "    pitch_val = np.mean(f0) if len(f0) > 0 else 0.0\n",
        "    jitter = (np.mean(np.abs(np.diff(f0))) / pitch_val) if pitch_val > 0 else 0.0\n",
        "    shimmer = 0.0\n",
        "    formants_vec = np.zeros(5)\n",
        "    vq_features = np.array([jitter, shimmer, pitch_val])\n",
        "\n",
        "    # [cite_start]Concatenate [cite: 1671]\n",
        "    return np.concatenate([mfcc_global, rms_global, zcr, centroid, lpcc_global, wpt_global, rolloff_global, kurtosis, vq_features, formants_vec])\n",
        "\n",
        "def _fix_length(arr, target_len):\n",
        "    if len(arr) < target_len: return np.pad(arr, (0, target_len - len(arr)))\n",
        "    return arr[:target_len]\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN PROCESSING LOOP\n",
        "# ==========================================\n",
        "def process_emodb_data():\n",
        "    X_features = []\n",
        "    Y_labels = []\n",
        "    S_speakers = []\n",
        "\n",
        "    print(f\"Reading files from: {DATASET_PATH}\")\n",
        "\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        print(\"Error: Dataset path does not exist.\")\n",
        "        return\n",
        "\n",
        "    files = [f for f in os.listdir(DATASET_PATH) if f.endswith('.wav')]\n",
        "    print(f\"Found {len(files)} .wav files.\")\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # <--- TQDM ADDED HERE: This tracks the main bottleneck (file processing)\n",
        "    for file_name in tqdm(files, desc=\"Extracting Features\", unit=\"file\"):\n",
        "        file_path = os.path.join(DATASET_PATH, file_name)\n",
        "\n",
        "        try:\n",
        "            # 1. Extract Info from Filename\n",
        "            speaker_id = file_name[0:2]\n",
        "            emotion_code = file_name[5]\n",
        "\n",
        "            # 2. Validate Emotion Code\n",
        "            if emotion_code not in CODE_TO_EMOTION:\n",
        "                # Use tqdm.write so the print doesn't break the progress bar\n",
        "                tqdm.write(f\"Skipping {file_name}: Unknown code '{emotion_code}'\")\n",
        "                continue\n",
        "\n",
        "            emotion_name = CODE_TO_EMOTION[emotion_code]\n",
        "            label_int = EMOTION_TO_INT[emotion_name]\n",
        "\n",
        "            # 3. Extract Features (This takes the most time)\n",
        "            features = extract_bhangale_features(file_path)\n",
        "\n",
        "            # 4. Store\n",
        "            if features.shape[0] == 715:\n",
        "                X_features.append(features)\n",
        "                Y_labels.append(label_int)\n",
        "                S_speakers.append(speaker_id)\n",
        "                count += 1\n",
        "            else:\n",
        "                tqdm.write(f\"Error shape {features.shape} in {file_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. SAVE ARRAYS\n",
        "    # ==========================================\n",
        "    print(\"\\nConverting to Numpy Arrays...\")\n",
        "    X = np.array(X_features)\n",
        "    Y = np.array(Y_labels)\n",
        "    S = np.array(S_speakers)\n",
        "\n",
        "    # Reshape X for the 1D CNN: (Batch, 715, 1)\n",
        "    X = X[..., np.newaxis]\n",
        "\n",
        "    print(f\"Processed: {count} files\")\n",
        "    print(f\"X Shape: {X.shape}\")\n",
        "    print(f\"Y Shape: {Y.shape}\")\n",
        "    print(f\"Speakers: {len(np.unique(S))}\")\n",
        "\n",
        "    if not os.path.exists(OUTPUT_PATH):\n",
        "        os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "    print(f\"Saving .npy files to {OUTPUT_PATH}...\")\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"X_emodb.npy\"), X)\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"Y_emodb.npy\"), Y)\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"S_emodb.npy\"), S)\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_emodb_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZobIGWKJxpw",
        "outputId": "c2253165-3d0d-4d55-ab87-2f3dc543f03b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files from: /content/drive/MyDrive/DeepLearning/External/EMoDB/\n",
            "Found 535 .wav files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features:  16%|█▋        | 87/535 [02:01<09:01,  1.21s/file]/tmp/ipython-input-2224402440.py:57: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  kurtosis = _fix_length(scipy.stats.kurtosis(S, axis=0), 199)\n",
            "Extracting Features: 100%|██████████| 535/535 [11:27<00:00,  1.28s/file]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting to Numpy Arrays...\n",
            "Processed: 535 files\n",
            "X Shape: (535, 715, 1)\n",
            "Y Shape: (535,)\n",
            "Speakers: 10\n",
            "Saving .npy files to processed_data/...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntax: !zip -r <name_of_new_zip_file> <folder_to_zip>\n",
        "!zip -r processed_data.zip processed_data/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43DUM7ZBRtAn",
        "outputId": "7ca387c4-6f1d-4ac6-a6aa-ad8ff77b1131"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: processed_data/ (stored 0%)\n",
            "  adding: processed_data/Y_emodb.npy (deflated 88%)\n",
            "  adding: processed_data/X_emodb.npy (deflated 43%)\n",
            "  adding: processed_data/S_emodb.npy (deflated 90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. LOAD DATA\n",
        "# ==========================================\n",
        "DATA_PATH = \"processed_data/\"\n",
        "\n",
        "print(\"Loading data...\")\n",
        "X = np.load(os.path.join(DATA_PATH, \"X_emodb.npy\"))\n",
        "Y = np.load(os.path.join(DATA_PATH, \"Y_emodb.npy\"))\n",
        "\n",
        "# Verify Shapes\n",
        "# Input should be (535, 715, 1)\n",
        "# Output should be (535,)\n",
        "print(f\"Features: {X.shape}\")\n",
        "print(f\"Labels: {Y.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. REPLICATE PAPER SPLIT (70:30)\n",
        "# ==========================================\n",
        "# The paper states: \"The data is split in the ratio of 70:30 for training and testing\" [cite: 667]\n",
        "# We use 'stratify=Y' to ensure every emotion is represented fairly, just like a good paper would.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=Y\n",
        ")\n",
        "\n",
        "print(f\"Training Samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing Samples: {X_test.shape[0]}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. BUILD MODEL (Exact Architecture)\n",
        "# ==========================================\n",
        "# Reference: Table 1 and Section 3 [cite: 586-610, 626]\n",
        "\n",
        "def build_bhangale_1d_cnn():\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # --- Conv Block 1 ---\n",
        "    # \"Conv1 (Filters: 32, Filter Size: 1x3, Stride: 1, Padding: Yes) -> ReLU1\" [cite: 587]\n",
        "    model.add(layers.Conv1D(filters=32, kernel_size=3, strides=1, padding='same', input_shape=(715, 1)))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # --- Conv Block 2 ---\n",
        "    # \"Conv2 (Filters: 64, Filter Size: 1x3... Padding: Yes) -> ReLU2\" [cite: 589]\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # --- Conv Block 3 ---\n",
        "    # \"Conv3 (Filters: 128, Filter Size: 1x3... Padding: Yes) -> ReLU3\" [cite: 590]\n",
        "    model.add(layers.Conv1D(filters=128, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # --- Flatten ---\n",
        "    # \"Following the 3 CNN layers... Flattening\"\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # --- Fully Connected Layers ---\n",
        "    # \"2 fully connected layers are used having 20 and 7 hidden layers\" [cite: 604]\n",
        "\n",
        "    # FC 1 (20 Neurons)\n",
        "    model.add(layers.Dense(20))\n",
        "    model.add(layers.ReLU()) # Non-linear activation implied by Eq 37 [cite: 606]\n",
        "\n",
        "    # FC 2 (Output, 7 Neurons for EMODB)\n",
        "    model.add(layers.Dense(7))\n",
        "\n",
        "    # Softmax Classifier [cite: 610]\n",
        "    model.add(layers.Softmax())\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_bhangale_1d_cnn()\n",
        "model.summary()\n",
        "\n",
        "# Check trainable params. Paper says \"1.77 M\".\n",
        "# Our Summary should show approx ~1.7 to 1.8 Million.\n",
        "\n",
        "# ==========================================\n",
        "# 4. COMPILE & TRAIN\n",
        "# ==========================================\n",
        "# \"trained using stochastic gradient descent with momentum (SGDM)\" [cite: 622]\n",
        "# \"batch size of 64\" [cite: 623]\n",
        "# \"200 epochs\" [cite: 624]\n",
        "# \"initial learning rate of 0.001, and momentum of 0.9\" [cite: 624]\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy', # Used because Y is integers (0, 1, 2...), not one-hot vectors\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nStarting Training (Replication Mode)...\")\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    epochs=200,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, Y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 5. FINAL EVALUATION\n",
        "# ==========================================\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(f\"FINAL REPLICATION RESULT:\")\n",
        "print(f\"Target Accuracy (Paper): ~93.31%\")\n",
        "print(f\"Achieved Accuracy:       {test_acc*100:.2f}%\")\n",
        "print(\"------------------------------------------------\")\n",
        "\n",
        "# Plotting to verify convergence (optional)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Test')\n",
        "plt.title('Model Accuracy Replication')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4th2EG6JQu7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}