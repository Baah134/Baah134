{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT26o4An9H1XGI+JFZUD1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baah134/Baah134/blob/main/SER_CARINE/Paper_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa numpy scipy PyWavelets\n",
        "DATASET_PATH = \"/content/drive/MyDrive/DeepLearning/External/EMoDB/\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u07C48dbE7xq",
        "outputId": "67792a84-8339-4919-c8ba-443bd9462ace"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (1.9.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import scipy.stats\n",
        "import pywt\n",
        "from scipy.signal import lfilter\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP & CONFIGURATION\n",
        "# ==========================================\n",
        "# Path from your code snippet\n",
        "DATASET_PATH = \"/content/drive/MyDrive/DeepLearning/External/EMoDB/\"\n",
        "OUTPUT_PATH = \"processed_data/\"\n",
        "\n",
        "# The exact 7 classes derived from your logic\n",
        "CLASSES = ['Angry', 'Boredom', 'Disgust', 'Anxiety', 'Happiness', 'Sadness', 'Neutral']\n",
        "\n",
        "# The \"Decoder Ring\" (Filename Code -> Emotion Name)\n",
        "CODE_TO_EMOTION = {\n",
        "    'W': 'Angry',\n",
        "    'L': 'Boredom',\n",
        "    'E': 'Disgust',\n",
        "    'A': 'Anxiety', # Often 'Fear' in literature, but 'Anxiety' in your code\n",
        "    'F': 'Happiness',\n",
        "    'T': 'Sadness',\n",
        "    'N': 'Neutral'\n",
        "}\n",
        "\n",
        "# Map Emotion Name -> Integer (0, 1, 2...) for the model\n",
        "EMOTION_TO_INT = {label: i for i, label in enumerate(CLASSES)}\n",
        "\n",
        "# ==========================================\n",
        "# 2. FEATURE EXTRACTOR (BHANGALE ET AL.)\n",
        "# ==========================================\n",
        "def extract_bhangale_features(audio_path):\n",
        "    \"\"\"\n",
        "    Extracts the exact 715-dim feature vector described in Bhangale et al. (2023).\n",
        "    Includes Pre-emphasis, MFCCs, ZCR, Spectral Features, and Wavelets.\n",
        "    \"\"\"\n",
        "    # Load audio, Resample to 16kHz [cite: 1795]\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Standardize to 4 seconds (64000 samples) [cite: 1796]\n",
        "    target_length = 64000\n",
        "    if len(y) < target_length:\n",
        "        y = np.pad(y, (0, target_length - len(y)))\n",
        "    else:\n",
        "        y = y[:target_length]\n",
        "\n",
        "    # Pre-emphasis filter\n",
        "    y = lfilter([1, -0.97], [1], y)\n",
        "\n",
        "    # Frame Settings: 40ms window, 50% overlap [cite: 1530]\n",
        "    n_fft = 640\n",
        "    hop_length = 320\n",
        "\n",
        "    # --- A. TIME-SERIES FEATURES (Length 199 each) ---\n",
        "    zcr = _fix_length(librosa.feature.zero_crossing_rate(y, frame_length=n_fft, hop_length=hop_length)[0], 199)\n",
        "    centroid = _fix_length(librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)[0], 199)\n",
        "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
        "    kurtosis = _fix_length(scipy.stats.kurtosis(S, axis=0), 199)\n",
        "\n",
        "    # --- B. STATIC FEATURES ---\n",
        "    # MFCC (39)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=n_fft, hop_length=hop_length)\n",
        "    mfcc_combined = np.concatenate((mfcc, librosa.feature.delta(mfcc), librosa.feature.delta(mfcc, order=2)), axis=0)\n",
        "    mfcc_global = np.mean(mfcc_combined, axis=1)\n",
        "\n",
        "    # Scalars & Stats\n",
        "    rms_global = np.array([np.mean(librosa.feature.rms(y=y, frame_length=n_fft, hop_length=hop_length)[0])])\n",
        "    rolloff_global = np.array([np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length)[0])])\n",
        "\n",
        "    # LPCC (13)\n",
        "    lpc_coeffs = librosa.lpc(y, order=13)\n",
        "    lpcc_global = lpc_coeffs[1:]\n",
        "    if len(lpcc_global) < 13: lpcc_global = np.pad(lpcc_global, (0, 13-len(lpcc_global)))\n",
        "\n",
        "    # Wavelet Packet Transform (56) [cite: 1636]\n",
        "    wp = pywt.WaveletPacket(data=y, wavelet='db2', mode='symmetric', maxlevel=3)\n",
        "    wpt_features = []\n",
        "    for node in wp.get_level(3, 'natural'):\n",
        "        d = node.data\n",
        "        wpt_features.extend([np.mean(d), np.median(d), np.std(d), np.var(d), scipy.stats.skew(d), scipy.stats.kurtosis(d), np.sum(d**2)])\n",
        "    wpt_global = np.array(wpt_features)\n",
        "\n",
        "    # Voice Quality (3) & Formants (5)\n",
        "    f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "    f0 = f0[~np.isnan(f0)]\n",
        "    pitch_val = np.mean(f0) if len(f0) > 0 else 0.0\n",
        "    jitter = (np.mean(np.abs(np.diff(f0))) / pitch_val) if pitch_val > 0 else 0.0\n",
        "    shimmer = 0.0 # Placeholder\n",
        "    formants_vec = np.zeros(5) # Placeholder\n",
        "    vq_features = np.array([jitter, shimmer, pitch_val])\n",
        "\n",
        "    # Concatenate [cite: 1671]\n",
        "    return np.concatenate([mfcc_global, rms_global, zcr, centroid, lpcc_global, wpt_global, rolloff_global, kurtosis, vq_features, formants_vec])\n",
        "\n",
        "def _fix_length(arr, target_len):\n",
        "    if len(arr) < target_len: return np.pad(arr, (0, target_len - len(arr)))\n",
        "    return arr[:target_len]\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN PROCESSING LOOP\n",
        "# ==========================================\n",
        "def process_emodb_data():\n",
        "    X_features = []\n",
        "    Y_labels = []\n",
        "    S_speakers = [] # Important for your research pivot!\n",
        "\n",
        "    print(f\"Reading files from: {DATASET_PATH}\")\n",
        "    files = os.listdir(DATASET_PATH)\n",
        "\n",
        "    count = 0\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(DATASET_PATH, file_name)\n",
        "\n",
        "        # 1. Filter out folders or non-wav files\n",
        "        if not os.path.isfile(file_path) or not file_name.endswith('.wav'):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # 2. Extract Info from Filename (EMODB Format: 03a01Wa.wav)\n",
        "            # Index 0-1: Speaker ID (03)\n",
        "            # Index 5: Emotion Code (W)\n",
        "\n",
        "            speaker_id = file_name[0:2]\n",
        "            emotion_code = file_name[5]\n",
        "\n",
        "            # 3. Validate Emotion Code\n",
        "            if emotion_code not in CODE_TO_EMOTION:\n",
        "                print(f\"Skipping {file_name}: Unknown code '{emotion_code}'\")\n",
        "                continue\n",
        "\n",
        "            emotion_name = CODE_TO_EMOTION[emotion_code]\n",
        "            label_int = EMOTION_TO_INT[emotion_name]\n",
        "\n",
        "            # 4. Extract Features\n",
        "            features = extract_bhangale_features(file_path)\n",
        "\n",
        "            # 5. Store\n",
        "            if features.shape[0] == 715:\n",
        "                X_features.append(features)\n",
        "                Y_labels.append(label_int)\n",
        "                S_speakers.append(speaker_id)\n",
        "                count += 1\n",
        "            else:\n",
        "                print(f\"Error shape {features.shape} in {file_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. SAVE ARRAYS\n",
        "    # ==========================================\n",
        "    X = np.array(X_features)\n",
        "    Y = np.array(Y_labels)\n",
        "    S = np.array(S_speakers)\n",
        "\n",
        "    # Reshape X for the 1D CNN: (Batch, 715, 1) [cite: 1716]\n",
        "    X = X[..., np.newaxis]\n",
        "\n",
        "    print(f\"\\n--- DONE ---\")\n",
        "    print(f\"Processed: {count} files\")\n",
        "    print(f\"X Shape: {X.shape}\") # Should be (535, 715, 1)\n",
        "    print(f\"Y Shape: {Y.shape}\")\n",
        "    print(f\"Speakers Extracted: {len(np.unique(S))}\") # Should be 10 for EMODB\n",
        "\n",
        "    if not os.path.exists(OUTPUT_PATH):\n",
        "        os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"X_emodb.npy\"), X)\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"Y_emodb.npy\"), Y)\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"S_emodb.npy\"), S) # Save speakers for your custom split later\n",
        "    print(f\"Saved .npy files to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_emodb_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtviNJ2mFHxH",
        "outputId": "73ea8f55-29d8-4e4e-ca2f-e0d8ca492585"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading files from: /content/drive/MyDrive/DeepLearning/External/EMoDB/\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1631950075.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  kurtosis = _fix_length(scipy.stats.kurtosis(S, axis=0), 199)\n"
          ]
        }
      ]
    }
  ]
}