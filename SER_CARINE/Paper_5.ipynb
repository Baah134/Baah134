{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YTOkhu-46vVC1b5AfreHItzYStk9wMSx",
      "authorship_tag": "ABX9TyPvrGJCHCoa7Jh0h5Wfegfm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baah134/Baah134/blob/main/SER_CARINE/Paper_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FEATURE EXTRACTION**"
      ],
      "metadata": {
        "id": "izxIMhHb3JHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB-M96RKq9p8",
        "outputId": "5c49da3d-3a33-41d6-c10b-b3491ba851a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Indexing Files ---\n",
            "Parsing TESS...\n",
            "Parsing SAVEE...\n",
            "Parsing RAVDESS...\n",
            "\n",
            "Dataset Summary:\n",
            "Source\n",
            "TESS       2800\n",
            "RAVDESS    1440\n",
            "SAVEE       480\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Emotion Distribution:\n",
            "Emotion\n",
            "angry       652\n",
            "sad         652\n",
            "disgust     652\n",
            "fear        652\n",
            "surprise    652\n",
            "happy       652\n",
            "neutral     616\n",
            "calm        192\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Phase 2: Processing Audio (This may take a while) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4720/4720 [31:13<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Phase 3: saving to .npy ---\n",
            "Final X Shape: (9440, 40, 216, 1)\n",
            "Final Y Shape: (9440,)\n",
            "Final S Shape: (9440,)\n",
            "\n",
            "Success! Data saved to Singh_Processed_Data/\n",
            "Ready for 'Strict Speaker Split' Training.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION (Singh et al. 2023 Specs)\n",
        "# ==========================================\n",
        "# Paths - UPDATE THESE IF NEEDED\n",
        "BASE_PATHS = {\n",
        "    \"TESS\": \"/content/drive/MyDrive/DeepLearning/External/Toronto Emotional Speech Set/TESS/TESS/\",\n",
        "    \"SAVEE\": \"/content/drive/MyDrive/DeepLearning/External/SAVEE/\",\n",
        "    \"RAVDESS\": \"/content/drive/MyDrive/DeepLearning/External/RAVDESS Emotional Speech Audio/audio_speech_actors_01-24/\"\n",
        "}\n",
        "\n",
        "OUTPUT_PATH = \"Singh_Processed_Data/\"\n",
        "\n",
        "# Audio Parameters (From Paper)\n",
        "SAMPLE_RATE = 44100        # High fidelity as per paper\n",
        "DURATION_SECONDS = 2.5     # Fixed duration\n",
        "TOTAL_SAMPLES = int(SAMPLE_RATE * DURATION_SECONDS) # 110,250 samples\n",
        "\n",
        "# Feature Extraction\n",
        "N_MFCC = 40                # Standard for CNN inputs\n",
        "HOP_LENGTH = 512\n",
        "N_FFT = 2048\n",
        "\n",
        "# Augmentation\n",
        "NOISE_FACTOR = 0.005       # Amplitude of white noise\n",
        "\n",
        "# Labels (0-7)\n",
        "EMOTION_MAP = {\n",
        "    'neutral': 0,\n",
        "    'calm': 1,\n",
        "    'happy': 2,\n",
        "    'sad': 3,\n",
        "    'angry': 4,\n",
        "    'fear': 5,\n",
        "    'disgust': 6,\n",
        "    'surprise': 7\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PARSERS (Path & Speaker Extraction)\n",
        "# ==========================================\n",
        "\n",
        "def parse_tess(base_path):\n",
        "    print(\"Parsing TESS...\")\n",
        "    data = []\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Warning: TESS path not found: {base_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # TESS structure: /TESS/OAF_Fear/OAF_fear_01.wav\n",
        "    for folder in os.listdir(base_path):\n",
        "        folder_path = os.path.join(base_path, folder)\n",
        "        if not os.path.isdir(folder_path): continue\n",
        "\n",
        "        for file in os.listdir(folder_path):\n",
        "            if not file.endswith('.wav'): continue\n",
        "\n",
        "            # Logic from your snippet + Speaker Extraction\n",
        "            # Filename: OAF_fear_01.wav\n",
        "            parts = file.split('.')[0].split('_')\n",
        "\n",
        "            # Speaker: 'OAF' or 'YAF' (Always 1st part)\n",
        "            speaker = f\"TESS_{parts[0]}\"\n",
        "\n",
        "            # Emotion: usually 2nd part, handle 'ps' for surprise\n",
        "            emotion_code = parts[2] if len(parts) > 2 else parts[1] # Robustness check\n",
        "\n",
        "            if emotion_code == 'ps':\n",
        "                emotion = 'surprise'\n",
        "            else:\n",
        "                emotion = emotion_code.lower()\n",
        "\n",
        "            if emotion in EMOTION_MAP:\n",
        "                data.append({\n",
        "                    'Path': os.path.join(folder_path, file),\n",
        "                    'Emotion': emotion,\n",
        "                    'SpeakerID': speaker,\n",
        "                    'Source': 'TESS'\n",
        "                })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def parse_savee(base_path):\n",
        "    print(\"Parsing SAVEE...\")\n",
        "    data = []\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Warning: SAVEE path not found: {base_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # SAVEE structure: Sometimes flat, sometimes folders (DC, JE, JK, KL)\n",
        "    # We scan recursively to be safe\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if not file.endswith('.wav'): continue\n",
        "\n",
        "            # Speaker Logic: Usually folder name or filename prefix\n",
        "            # Assuming folder structure: /SAVEE/DC/a01.wav -> Speaker DC\n",
        "            folder_name = os.path.basename(root)\n",
        "\n",
        "            if folder_name in ['DC', 'JE', 'JK', 'KL']:\n",
        "                speaker = f\"SAVEE_{folder_name}\"\n",
        "            else:\n",
        "                # If flat file: DC_a01.wav (Extract first 2 chars)\n",
        "                # Or if just a01.wav, we might lose speaker info without folder.\n",
        "                # Assuming standard SAVEE download often has prefix.\n",
        "                # Fallback: Use folder name even if generic\n",
        "                speaker = f\"SAVEE_{folder_name}\"\n",
        "\n",
        "            # Emotion Logic (from your snippet)\n",
        "            # Filename: a01.wav -> 'a' is anger\n",
        "            # Remove digits and extension\n",
        "            filename_clean = file.split('.')[0] # a01\n",
        "            code = ''.join([i for i in filename_clean if not i.isdigit() and i != '_'])\n",
        "\n",
        "            # Handle SAVEE specific codes (e.g. 'sa' vs 'a')\n",
        "            # Extract last part if there's a prefix like DC_a01\n",
        "            if '_' in file:\n",
        "                code = file.split('_')[1]\n",
        "                code = ''.join([i for i in code.split('.')[0] if not i.isdigit()])\n",
        "\n",
        "            emo_lookup = {\n",
        "                'a': 'angry', 'd': 'disgust', 'f': 'fear',\n",
        "                'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprise'\n",
        "            }\n",
        "\n",
        "            emotion = emo_lookup.get(code)\n",
        "\n",
        "            if emotion and emotion in EMOTION_MAP:\n",
        "                data.append({\n",
        "                    'Path': os.path.join(root, file),\n",
        "                    'Emotion': emotion,\n",
        "                    'SpeakerID': speaker,\n",
        "                    'Source': 'SAVEE'\n",
        "                })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def parse_ravdess(base_path):\n",
        "    print(\"Parsing RAVDESS...\")\n",
        "    data = []\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Warning: RAVDESS path not found: {base_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # RAVDESS structure: /Actor_01/03-01-05-01-01-01-01.wav\n",
        "    for actor_dir in os.listdir(base_path):\n",
        "        actor_path = os.path.join(base_path, actor_dir)\n",
        "        if not os.path.isdir(actor_path): continue\n",
        "\n",
        "        for file in os.listdir(actor_path):\n",
        "            if not file.endswith('.wav'): continue\n",
        "\n",
        "            parts = file.split('.')[0].split('-')\n",
        "            if len(parts) < 7: continue\n",
        "\n",
        "            # Emotion: 3rd part (Index 2)\n",
        "            # 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fear, 07=disgust, 08=surprise\n",
        "            emo_code = int(parts[2])\n",
        "            emo_map_rav = {\n",
        "                1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n",
        "                5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'\n",
        "            }\n",
        "            emotion = emo_map_rav.get(emo_code)\n",
        "\n",
        "            # Speaker: 7th part (Index 6) -> \"01\", \"24\"\n",
        "            actor_id = parts[6]\n",
        "            speaker = f\"RAV_{actor_id}\"\n",
        "\n",
        "            if emotion in EMOTION_MAP:\n",
        "                data.append({\n",
        "                    'Path': os.path.join(actor_path, file),\n",
        "                    'Emotion': emotion,\n",
        "                    'SpeakerID': speaker,\n",
        "                    'Source': 'RAVDESS'\n",
        "                })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# ==========================================\n",
        "# 3. SIGNAL PROCESSING FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def get_mfcc(y, sr):\n",
        "    \"\"\"Compute MFCCs and transpose to (Bands, Time)\"\"\"\n",
        "    # 1. MFCC Extraction\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
        "    # 2. Log Scale (Standard for Deep Learning)\n",
        "    # librosa.feature.mfcc returns already suitable values, but log-mel is often used.\n",
        "    # The paper mentions 'Cepstral Coefficients', standard implementation is usually fine.\n",
        "    return mfcc\n",
        "\n",
        "def process_audio_file(file_path):\n",
        "    \"\"\"\n",
        "    Reads, Resamples (44.1k), Pads/Truncates (2.5s), Augments.\n",
        "    Returns: List of MFCC arrays (Clean, Noisy)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Load & Resample (Critical: Singh et al. used 44.1 kHz)\n",
        "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "        # 2. Duration Standardization (2.5s)\n",
        "        if len(y) > TOTAL_SAMPLES:\n",
        "            # Truncate: take the center (most emotional part)\n",
        "            start = (len(y) - TOTAL_SAMPLES) // 2\n",
        "            y_fixed = y[start : start + TOTAL_SAMPLES]\n",
        "        else:\n",
        "            # Pad: Append zeros\n",
        "            padding = TOTAL_SAMPLES - len(y)\n",
        "            y_fixed = np.pad(y, (0, padding), 'constant')\n",
        "\n",
        "        # 3. Augmentation (White Noise)\n",
        "        noise_amp = NOISE_FACTOR * np.random.uniform() * np.amax(y_fixed)\n",
        "        y_noisy = y_fixed + noise_amp * np.random.normal(size=y_fixed.shape[0])\n",
        "\n",
        "        # 4. Feature Extraction\n",
        "        mfcc_clean = get_mfcc(y_fixed, sr)\n",
        "        mfcc_noisy = get_mfcc(y_noisy, sr)\n",
        "\n",
        "        return [mfcc_clean, mfcc_noisy]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN PIPELINE\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(OUTPUT_PATH):\n",
        "        os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "    # --- Step 1: Create Master DataFrame ---\n",
        "    print(\"--- Phase 1: Indexing Files ---\")\n",
        "    df_tess = parse_tess(BASE_PATHS[\"TESS\"])\n",
        "    df_savee = parse_savee(BASE_PATHS[\"SAVEE\"])\n",
        "    df_ravdess = parse_ravdess(BASE_PATHS[\"RAVDESS\"])\n",
        "\n",
        "    master_df = pd.concat([df_tess, df_savee, df_ravdess], ignore_index=True)\n",
        "\n",
        "    print(\"\\nDataset Summary:\")\n",
        "    print(master_df['Source'].value_counts())\n",
        "    print(\"\\nEmotion Distribution:\")\n",
        "    print(master_df['Emotion'].value_counts())\n",
        "\n",
        "    # --- Step 2: Processing Loop ---\n",
        "    print(\"\\n--- Phase 2: Processing Audio (This may take a while) ---\")\n",
        "\n",
        "    X_data = []\n",
        "    Y_data = []\n",
        "    S_data = [] # Speaker IDs\n",
        "\n",
        "    # Using tqdm for progress bar\n",
        "    for index, row in tqdm(master_df.iterrows(), total=master_df.shape[0]):\n",
        "        path = row['Path']\n",
        "        emotion_str = row['Emotion']\n",
        "        speaker_id = row['SpeakerID']\n",
        "\n",
        "        # Convert Emotion String to Int\n",
        "        label = EMOTION_MAP[emotion_str]\n",
        "\n",
        "        # Process (Get Clean and Noisy versions)\n",
        "        features = process_audio_file(path)\n",
        "\n",
        "        if features is not None:\n",
        "            # Append Clean\n",
        "            X_data.append(features[0])\n",
        "            Y_data.append(label)\n",
        "            S_data.append(speaker_id)\n",
        "\n",
        "            # Append Noisy (Augmentation)\n",
        "            X_data.append(features[1])\n",
        "            Y_data.append(label)\n",
        "            S_data.append(speaker_id) # Same speaker for augmented file!\n",
        "\n",
        "    # --- Step 3: Final Formatting & Saving ---\n",
        "    print(\"\\n--- Phase 3: saving to .npy ---\")\n",
        "\n",
        "    # Convert to Numpy Arrays\n",
        "    X = np.array(X_data)\n",
        "    Y = np.array(Y_data)\n",
        "    S = np.array(S_data)\n",
        "\n",
        "    # Reshape X for CNN: (N, 40, Time, 1)\n",
        "    # Current shape is (N, 40, Time). We add the channel dimension.\n",
        "    X = X[..., np.newaxis]\n",
        "\n",
        "    print(f\"Final X Shape: {X.shape}\")\n",
        "    print(f\"Final Y Shape: {Y.shape}\")\n",
        "    print(f\"Final S Shape: {S.shape}\")\n",
        "\n",
        "    # Save\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"X_singh.npy\"), X)\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"Y_singh.npy\"), Y)\n",
        "    np.save(os.path.join(OUTPUT_PATH, \"S_singh.npy\"), S)\n",
        "\n",
        "    print(f\"\\nSuccess! Data saved to {OUTPUT_PATH}\")\n",
        "    print(\"Ready for 'Strict Speaker Split' Training.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r Singh_Processed_Data.zip Singh_Processed_Data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCnkad0hrz-8",
        "outputId": "fd69ed62-80b7-4fe6-b907-0e0985167b05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Singh_Processed_Data/ (stored 0%)\n",
            "  adding: Singh_Processed_Data/X_singh.npy (deflated 27%)\n",
            "  adding: Singh_Processed_Data/Y_singh.npy (deflated 98%)\n",
            "  adding: Singh_Processed_Data/S_singh.npy (deflated 100%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Singh_Processed_Data.zip -d ./"
      ],
      "metadata": {
        "id": "afv_4HyAsTdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RANDOM SPLIT**"
      ],
      "metadata": {
        "id": "piXU6gHM3QAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks, backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "DATA_PATH = \"Singh_Processed_Data/\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "LEARNING_RATE = 0.001\n",
        "TEST_SIZE = 0.25 # Paper specified 75/25 split (Section 3.2)\n",
        "\n",
        "EMOTION_LABELS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fear', 'disgust', 'surprise']\n",
        "NUM_CLASSES = len(EMOTION_LABELS)\n",
        "\n",
        "# ==========================================\n",
        "# 2. PREPROCESSING HELPERS (NEW)\n",
        "# ==========================================\n",
        "def clean_and_normalize(X_train, X_test):\n",
        "    \"\"\"\n",
        "    1. Replaces NaNs/Infs with 0.\n",
        "    2. Applies Standard Scaling (Zero Mean, Unit Variance) based on TRAIN stats.\n",
        "    \"\"\"\n",
        "    print(\"Sanitizing data (NaN check)...\")\n",
        "    # 1. Replace NaNs and Infinity with 0\n",
        "    X_train = np.nan_to_num(X_train)\n",
        "    X_test = np.nan_to_num(X_test)\n",
        "\n",
        "    print(\"Normalizing data (StandardScaler)...\")\n",
        "    # 2. Calculate Mean and Std from TRAIN set only (Shape: N, 40, 216, 1)\n",
        "    # We normalize across the entire dataset (global scaling) or per feature channel.\n",
        "    # For CNNs, global scaling or per-frequency-bin scaling is common.\n",
        "    # Here we calculate statistics for each MFCC bin (axis 1) across all samples and time.\n",
        "\n",
        "    # Mean/Std shape: (1, 40, 1, 1) broadcastable\n",
        "    mean = np.mean(X_train, axis=(0, 2, 3), keepdims=True)\n",
        "    std = np.std(X_train, axis=(0, 2, 3), keepdims=True)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    std = np.where(std == 0, 1.0, std)\n",
        "\n",
        "    X_train_norm = (X_train - mean) / std\n",
        "    X_test_norm = (X_test - mean) / std\n",
        "\n",
        "    return X_train_norm, X_test_norm\n",
        "\n",
        "# ==========================================\n",
        "# 3. MODEL ARCHITECTURE (Singh et al. 2023)\n",
        "# ==========================================\n",
        "class SequenceAttention(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Attention Layer that allows the model to weight the sequence\n",
        "    before passing it to the final LSTM.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SequenceAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='glorot_uniform', trainable=True)\n",
        "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
        "        super(SequenceAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # x shape: (batch, time, features)\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        e = K.squeeze(e, axis=-1)\n",
        "        alpha = K.softmax(e)\n",
        "        alpha = K.expand_dims(alpha, axis=-1)\n",
        "        return x * alpha\n",
        "\n",
        "def build_singh_model(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # --- PART A: CNN-2D Blocks (Spatial) ---\n",
        "    x = inputs\n",
        "    # 4 Identical Convolution Blocks\n",
        "    for _ in range(4):\n",
        "        x = layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.MaxPooling2D((2, 2))(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # --- PART B: Reshape for LSTM ---\n",
        "    # Permute to (Batch, Time, Freq, Channels)\n",
        "    x = layers.Permute((2, 1, 3))(x)\n",
        "    # Flatten Freq and Channels to get (Batch, Time, Features)\n",
        "    bs, t, f, c = x.shape\n",
        "    x = layers.Reshape((-1, x.shape[2] * x.shape[3]))(x)\n",
        "\n",
        "    # --- PART C: LSTM + Attention (Temporal) ---\n",
        "    x = layers.LSTM(32, return_sequences=True)(x)\n",
        "    x = SequenceAttention()(x) # Weights the important time steps\n",
        "    x = layers.LSTM(32, return_sequences=False)(x) # Consolidates to vector\n",
        "\n",
        "    # --- PART D: Classifier ---\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name=\"Singh_Replication_Model\")\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN EXECUTION\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Load Data\n",
        "    print(\"Loading Data...\")\n",
        "    try:\n",
        "        X = np.load(os.path.join(DATA_PATH, \"X_singh.npy\"))\n",
        "        y = np.load(os.path.join(DATA_PATH, \"Y_singh.npy\"))\n",
        "        print(f\"Loaded Data: X={X.shape}, Y={y.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Data not found in {DATA_PATH}. Run the feature extraction script first.\")\n",
        "        exit()\n",
        "\n",
        "    # 2. Random Split (75% Train, 25% Test)\n",
        "    print(f\"Splitting Data (Test Size: {TEST_SIZE})...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42, shuffle=True)\n",
        "\n",
        "    # 3. CLEAN AND NORMALIZE (Added Step)\n",
        "    X_train, X_test = clean_and_normalize(X_train, X_test)\n",
        "\n",
        "    print(f\"Train Set: {X_train.shape}\")\n",
        "    print(f\"Test Set:  {X_test.shape}\")\n",
        "\n",
        "    # 4. Build Model\n",
        "    input_shape = X_train.shape[1:]\n",
        "    model = build_singh_model(input_shape, NUM_CLASSES)\n",
        "    model.summary()\n",
        "\n",
        "    # 5. Compile\n",
        "    optimizer = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 6. Callbacks\n",
        "    checkpoint = callbacks.ModelCheckpoint(\"best_singh_model.keras\", monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # 7. Train\n",
        "    print(\"\\nStarting Replication Training...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[checkpoint, early_stop, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # 8. Evaluation & Viz\n",
        "    print(\"\\nEvaluating Model...\")\n",
        "    loss, acc = model.evaluate(X_test, y_test)\n",
        "    print(f\"\\nREPLICATION RESULT: Test Accuracy = {acc*100:.2f}%\")\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=EMOTION_LABELS))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMOTION_LABELS, yticklabels=EMOTION_LABELS)\n",
        "    plt.title('Confusion Matrix (Normalized)')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Test')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gqPGHGiqvaJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STRICT SPEAKER SPLIT**"
      ],
      "metadata": {
        "id": "KZNySKi93Wek"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqEI-l0P3dOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}