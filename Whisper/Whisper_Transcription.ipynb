{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZ3h+2K/NdSgSet7xrAARs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baah134/Baah134/blob/main/Whisper/Whisper_Transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "EnPom6umE-5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "print(datasets.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65onAyfoEPak",
        "outputId": "55524d73-6861-432a-a17d-fb38b8f43dc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "afrispeech = load_dataset(\"tobiolatunji/afrispeech-200\", 'all', split=\"train\", streaming=True)\n",
        "\n",
        "print(next(iter(afrispeech)))\n",
        "print(list(afrispeech.take(100)))\n"
      ],
      "metadata": {
        "id": "QzthGgC1H-io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in afrispeech.take(1):  # Use .take(1) to get the first sample\n",
        "    audio = sample[\"audio\"]  # Access the audio data\n",
        "    transcription = sample[\"transcript\"]  # Access the transcription\n",
        "\n",
        "    # Display information\n",
        "    print(\"Audio Path:\", audio[\"path\"])  # Path to the audio file (if available)\n",
        "    print(\"Audio Data:\", audio[\"array\"][:10], \"...\")  # Preview of audio waveform\n",
        "    print(\"Sampling Rate:\", audio[\"sampling_rate\"])  # Sampling rate of the audio\n",
        "    print(\"Transcription:\", transcription)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0YBg_tSI0z7",
        "outputId": "d8e9ac9c-ee7f-4b48-8306-2ff6d96c5eef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading metadata...: 57819it [00:01, 43453.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio Path: e54c96e9-6085-46a8-82c3-0c7efe10360d/bcc0988c06ec27f8622bc259c8d67a3d.wav\n",
            "Audio Data: [-1.95312500e-03 -1.73950195e-03 -1.55639648e-03 -1.31225586e-03\n",
            " -1.12915039e-03 -9.15527344e-04 -7.32421875e-04 -5.18798828e-04\n",
            " -3.05175781e-04 -9.15527344e-05] ...\n",
            "Sampling Rate: 44100\n",
            "Transcription: Similarly, an ACEI or a sartan is preferred over other antihypertensive drugs in diabetic patients where they slow the progression of nephropathy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n"
      ],
      "metadata": {
        "id": "Q5RSSmLWQqqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "-1QZd-5FRagl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer\n",
        "import whisper\n",
        "import numpy as np\n",
        "\n",
        "model = whisper.load_model(\"medium\")\n",
        "total_samples = 5\n",
        "wer_scores = []\n",
        "\n",
        "for sample in afrispeech.take(total_samples):\n",
        "    # Ground truth\n",
        "    ground_truth = sample['transcript']\n",
        "\n",
        "    # Audio waveform (convert to float32)\n",
        "    audio_array = np.array(sample['audio']['array'], dtype=np.float32)\n",
        "\n",
        "    # Transcribe the audio\n",
        "    result = model.transcribe(audio_array, fp16=False)\n",
        "    predicted_text = result['text']\n",
        "\n",
        "    # Calculate WER for the sample\n",
        "    error_rate = wer(ground_truth, predicted_text)\n",
        "    wer_scores.append(error_rate)\n",
        "\n",
        "    # Print results for the sample\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Predicted Text: {predicted_text}\")\n",
        "    print(f\"WER: {error_rate}\\n\")\n",
        "\n",
        "# Calculate average WER across samples\n",
        "average_wer = sum(wer_scores) / len(wer_scores)\n",
        "print(f\"Average WER across {total_samples} samples: {average_wer}\")"
      ],
      "metadata": {
        "id": "KOy2_Qp-RzOx",
        "outputId": "ec5f11aa-9b90-4aa0-c9d5-ed0c7d466030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:19<00:00, 77.3MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Reading metadata...: 57819it [00:01, 37835.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth: Similarly, an ACEI or a sartan is preferred over other antihypertensive drugs in diabetic patients where they slow the progression of nephropathy.\n",
            "Predicted Text: \n",
            "WER: 1.0\n",
            "\n",
            "Ground Truth: The moisture penetrates the sterile cloth, or paper, and carries organisms by capillary action to contaminate the eld.\n",
            "Predicted Text: \n",
            "WER: 1.0\n",
            "\n",
            "Ground Truth: Various sites may be used for subcutaneous injections, including the outer aspect of the upper arm, the abdomen from below the costal margin to the iliac crests, the anterior aspects of the thigh, the upper back and the upper ventral gluteal area.\n",
            "Predicted Text: \n",
            "WER: 1.0\n",
            "\n",
            "Ground Truth: The major causes are as follows:iSystemic hypertensioni i Mitral or aortic valve disease stenosisiii Ischaemic heart diseaseiv Myocardial diseases e.\n",
            "\n",
            "Predicted Text:  Erase Her reaches or cheeks Her Lungs the quicksand twitching may becomeaja purposes of healing\n",
            "WER: 1.0\n",
            "\n",
            "Average WER across 5 samples: 1.0\n"
          ]
        }
      ]
    }
  ]
}