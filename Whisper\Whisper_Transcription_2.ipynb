{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baah134/Baah134/blob/main/Whisper%5CWhisper_Transcription_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGCdnjpCInO2",
        "outputId": "3e1fa287-e1a7-4240-8771-b065f0050fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4pr0pdt0iND",
        "outputId": "9f076e63-1d0e-4f22-9432-435162407c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📂 Paths\n",
        "AUDIO_DIR = '/content/drive/MyDrive/DeepLearning/Whisper/dev'  # Folder containing subfolders of audio files\n",
        "CSV_PATH = \"/content/drive/MyDrive/DeepLearning/Whisper/transcripts.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPlhrbY6wcSy",
        "outputId": "ada43938-1bc3-4f96-9711-64f50f75340b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio File: /content/drive/MyDrive/DeepLearning/Whisper/dev/78e2d9eb-5307-404a-b06a-667fab452a73/c3ffc9c2156a4c0a1f5d8f51792a5825.wav\n",
            "Transcription: Note the absence of lower esophageal sphincter tone.\n",
            "--------------------------------------------------\n",
            "Audio File: /content/drive/MyDrive/DeepLearning/Whisper/dev/a731f4e7-e33e-4cc6-acbb-3034370483bb/0a1849c065857e7c66cd950ee70bf757.wav\n",
            "Transcription: In the future, chemo prevention of supragingival plaque will depend on products that are effective, substantive, and safe.\n",
            "--------------------------------------------------\n",
            "Audio File: /content/drive/MyDrive/DeepLearning/Whisper/dev/6150d067-12af-47ef-96fa-5449a957241f/3ad17fafe3a3f5524d867a623704fee1.wav\n",
            "Transcription: Oki said the agency would no longer tolerate excuses by occupants of distressed buildings, who often cited emotional attachments as putative reasons for not being willing to vacate the distressed structures.\n",
            "--------------------------------------------------\n",
            "Audio File: /content/drive/MyDrive/DeepLearning/Whisper/dev/5a15f13f-0e99-4f1e-bc12-4300bbe01b2a/6aac1d5992216c9878187fdc1f782b03.wav\n",
            "Transcription: Looking back, no regret. The bible says, All God has created is good. I am satisfied with my condition. You may be looking at the tangible element of an association, I am looking at tangible and intangible element of the association.\n",
            "--------------------------------------------------\n",
            "Audio File: /content/drive/MyDrive/DeepLearning/Whisper/dev/70d6ca42-5a22-4c51-bd4b-7a0fa7cb2f3b/49473ce2872521d839153be72dc58241.wav\n",
            "Transcription: Despite its importance, 6.8 million people at risk dont complete the annual ACR test.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# 📂 Paths\n",
        "AUDIO_DIR = '/content/drive/MyDrive/DeepLearning/Whisper/dev'  # Folder containing subfolders of audio files\n",
        "CSV_PATH = \"/content/drive/MyDrive/DeepLearning/Whisper/transcripts.csv\"\n",
        "\n",
        "# 📌 Load transcription CSV\n",
        "df_transcriptions = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Ensure the CSV has correct column names\n",
        "AUDIO_COLUMN = \"audio_path\"  # Column name containing full paths in the CSV\n",
        "TEXT_COLUMN = \"transcript\"  # Column name for transcriptions\n",
        "\n",
        "# Step 1️⃣: Extract just the filename (e.g., \"audio_001.wav\") from the CSV audio paths\n",
        "df_transcriptions[\"filename\"] = df_transcriptions[AUDIO_COLUMN].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "# Convert CSV into a dictionary {filename: transcription}\n",
        "transcription_dict = dict(zip(df_transcriptions[\"filename\"], df_transcriptions[TEXT_COLUMN]))\n",
        "\n",
        "# Step 2️⃣: Get list of all .wav files inside subdirectories\n",
        "all_audio_files = []\n",
        "for root, dirs, files in os.walk(AUDIO_DIR):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            full_path = os.path.join(root, file)\n",
        "            all_audio_files.append(full_path)\n",
        "\n",
        "# Step 3️⃣: Extract filenames from actual audio file paths\n",
        "audio_dict = {os.path.basename(file): file for file in all_audio_files}  # {filename: full_path}\n",
        "\n",
        "# Step 4️⃣: Match filenames in CSV with actual audio file paths\n",
        "matched_audio_files = {audio_dict[f]: transcription_dict[f] for f in transcription_dict if f in audio_dict}\n",
        "\n",
        "# Step 5️⃣: Pick 100 random samples\n",
        "random.seed(42)\n",
        "selected_samples = random.sample(list(matched_audio_files.items()), 100)\n",
        "\n",
        "# 🎯 Print first 5 matched samples\n",
        "for audio_path, transcription in selected_samples[:5]:\n",
        "    print(f\"Audio File: {audio_path}\")\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqTRGlOh1Grn",
        "outputId": "27d1187a-0e37-4ff3-8ed1-3a2ef420a6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "Processing Audio: 100%|██████████| 100/100 [16:19<00:00,  9.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcriptions saved to transcription_results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper jiwer pandas tqdm\n",
        "\n",
        "\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from jiwer import wer\n",
        "\n",
        "# Load Whisper model (small for speed, you can use \"medium\" or \"large\" for better accuracy)\n",
        "model = whisper.load_model(\"medium\")\n",
        "\n",
        "# CSV Output Path\n",
        "CSV_OUTPUT = \"transcription_results.csv\"\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Loop through the selected 100 audio samples\n",
        "for audio_path, actual_transcription in tqdm(selected_samples, desc=\"Processing Audio\"):\n",
        "\n",
        "    # Transcribe audio using Whisper\n",
        "    transcription_result = model.transcribe(audio_path)\n",
        "    whisper_transcription = transcription_result[\"text\"]\n",
        "\n",
        "    # Calculate Word Error Rate (WER)\n",
        "    error_rate = wer(actual_transcription, whisper_transcription)\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        \"Audio File\": audio_path,\n",
        "        \"Actual Transcription\": actual_transcription,\n",
        "        \"Whisper Transcription\": whisper_transcription,\n",
        "        \"Word Error Rate\": error_rate\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Save to CSV\n",
        "df_results.to_csv(CSV_OUTPUT, index=False)\n",
        "\n",
        "print(f\"Transcriptions saved to {CSV_OUTPUT}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXqIZ92MgTlnCNO/FbVP98",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}